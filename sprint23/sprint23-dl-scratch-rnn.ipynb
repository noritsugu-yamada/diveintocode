{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "    n_nodes1:\n",
    "    n_nodes2:\n",
    "    ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "        後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)[np.newaxis, :]\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnn_SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        leyer.W_h = self.lr * layer.dW_h\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    sigmoid関数の処理と導関数の算出\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A  = A\n",
    "        Z = 1/1+np.exp(-self.A)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        Z = self.forward(self.A)\n",
    "        dout_sig = Z*(1-Z)*dout\n",
    "        \n",
    "        return dout_sig\n",
    "    \n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    tanh関数の処理と導関数の算出\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = (np.exp(self.A)-np.exp(-self.A)) / (np.exp(self.A)+np.exp(-self.A))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        Z = self.forward(self.A)\n",
    "        dout_tanh = (1-Z**2)*dout\n",
    "        \n",
    "        return dout_tanh\n",
    "    \n",
    "class Relu:\n",
    "    \"\"\"\n",
    "    relu関数の処理と導関数の算出\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.where(self.A<=0, 0, self.A)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout_relu = np.where(self.A<=0, 0, 1)*dout\n",
    "        \n",
    "        return dout_relu\n",
    "    \n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    relu関数の処理とsoftmax_with_cross_entropyの導関数の算出\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        #if A.ndim == 2:\n",
    "           # A = A.T\n",
    "           # A = A - np.max(A, axis=0)\n",
    "           # y = np.exp(A) / np.sum(np.exp(A), axis=0)\n",
    "           # return y.T\n",
    "        A = A - np.max(A)\n",
    "        Z = np.exp(A) / np.sum(np.exp(A),axis=1, keepdims=True)\n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        dout_soft_max = self.Z - y\n",
    "        \n",
    "        return dout_soft_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        # XとWの内積をとり、biasを加える\n",
    "        self.Z = copy.deepcopy(X) # Xが更新されないように\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 勾配を算出する\n",
    "        self.dB = np.average(dA)\n",
    "        self.dW = np.dot(self.Z.T, dA)/dA.shape[0]\n",
    "        \n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        self = self.optimizer.update(self)# FCクラスのself.W, self.B, self.dW, self.dBを用いて更新\n",
    "        \n",
    "        return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    randomに生成したWと同じ配列の要素でdroput_ratioを以下のものをFalseとして格納\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, X, train_flag=True):\n",
    "        if train_flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_ratio # *X.shapeはXと同じshapeにするため\n",
    "            return X*self.mask\n",
    "        \n",
    "        else:\n",
    "            return X*(1-self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        \n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def cross_entropy_error(self, y_pred, y):\n",
    "        cross_entropy_error = np.sum(-1*y*np.log(y_pred+1e-10),axis=1)\n",
    "        \n",
    "        return cross_entropy_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "\n",
    "【問題3】（アドバンス課題）バックプロパゲーションの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self,batch,n_sequences, n_features, n_nodes, initializer,optimizer):\n",
    "        self.batch = batch\n",
    "        self.sequences = n_sequences\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.h = np.zeros((batch, n_nodes))\n",
    "\n",
    "\n",
    "        self.W_x = self.initializer.W(self.n_features, self.n_nodes)\n",
    "        self.W_h = self.initializer.W(self.n_nodes, self.n_nodes)\n",
    "        self.B = np.array([1])\n",
    "        \n",
    "        self.dh = np.zeros_like(self.batch_size, self.n_nodes)\n",
    "        self.dX = None\n",
    "        self.dW_x = None\n",
    "        self.dW_h = None\n",
    "        self.dh = None\n",
    "        self.dB = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X 次の形のndarray (batch_size, n_sequens, n_features)\n",
    "        W_x : 入力に対する重み (n_features, n_nodes)\n",
    "        self.h : 次の形のndarray (batch_size, n_nodoes)\n",
    "        W_h : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "        \"\"\"\n",
    "        output_list = []\n",
    "        self.X = X\n",
    "        for t in range(self.sequences):\n",
    "            a = np.dot(X[:,t,:], self.W_x) + np.dot(self.h, self.W_h) + b\n",
    "            \n",
    "            tanh = Tanh()\n",
    "            z = tanh.forward(a)\n",
    "            self.h = z\n",
    "            output_list += [z]\n",
    "        \n",
    "        output_array = np.array(output_list).reshape(self.batch, self.sequences, self.n_nodes)\n",
    "        self.output_array = output_array\n",
    "        \n",
    "        return output_array\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        \"\"\"\n",
    "        dz: Affinからの勾配(batch_size, self.sequences, self.nodes)\n",
    "        dh: RNNからの勾配(batch_size, n_nodes)\n",
    "        self.dX: ndarray(batch_size, n_sequences, n_features)\n",
    "        self.dW_x: ndarray (n_features, n_nodes)\n",
    "        self.dh: ndarray(batch_size, n_nodes)\n",
    "        self.dW_h: ndarray(n_nodes, n_nodes)\n",
    "        self.dB: ndarray(1)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.dX = np.zeros(self.batch, self.sequences, self.features)\n",
    "        \n",
    "        for t in reversed(range(self.n_sequences)):\n",
    "            d = dz[:,t,:] + self.dh\n",
    "            da = d*(1-(self.output_array[:,t,:])**2)\n",
    "            self.dW_x = np.dot(self.X[:,t,:].T, da)/self.batch_size\n",
    "            self.dh = np.dot(da, self.W_h.T)/self.batch_size\n",
    "            self.dW_h = np.dot(self.h.T, da)/self.batch_size\n",
    "            self.dB = np.average(da)\n",
    "            self.dX[:,t,:] = np.dot(da, self.W_x.T)\n",
    "            dh = self.dh\n",
    "            \n",
    "            \n",
    "            self = self.optimizer.update(layer)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes))\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(batch_size, n_features, n_nodes, SimpleInitializer(), SGD(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rnn.forward(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題2】小さな配列でのフォワードプロパゲーションの実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes))\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,2,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]]\n",
      "[[0.76188798 0.76213958 0.76239095 0.76255841]]\n",
      "[[0.792209   0.8141834  0.83404912 0.84977719]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in range(n_sequences):\n",
    "    a = np.dot(x[:,t,:], w_x) + np.dot(h, w_h) + b\n",
    "    tanh = Tanh()\n",
    "    z = tanh.forward(a)\n",
    "    h = z\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
