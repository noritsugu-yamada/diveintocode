{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】学習済みの重みによる推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download YOLOv3 weights from YOLO website.\n",
    "\n",
    "Convert the Darknet YOLO model to a Keras model.\n",
    "\n",
    "Run YOLO detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image detection mode\n",
      " Ignoring remaining command line arguments: C:\\Users\\umini\\git-test\\diveintocode-ml\\sprint18\\yolov3\\keras-yolo3\\kaggle_simpson_testset\\edna_krabappel_39.jpg,\n",
      "model_data/yolo.h5 model, anchors, and classes loaded.\n",
      "Input image filename:C:\\Users\\umini\\git-test\\diveintocode-ml\\sprint18\\yolov3\\keras-yolo3\\kaggle_simpson_testset\\edna_krabappel_39.jpg\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.63 (40, 7) (140, 174)\n",
      "2.968064564477503\n"
     ]
    }
   ],
   "source": [
    "#downloadしたweight-fileを使って実行\n",
    "%run yolo_video.py --image --input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"problem3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】学習のためのファイルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path スペース x1,y1,x2,y2,class_categoryに annotation.txtを整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column名を付けてdf化\n",
    "columns = ['path','x1','y1','x2','y2','label']\n",
    "df = pd.read_csv('annotation.txt',names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abraham_grampa_simpson': 0,\n",
       " 'apu_nahasapeemapetilon': 1,\n",
       " 'bart_simpson': 2,\n",
       " 'charles_montgomery_burns': 3,\n",
       " 'chief_wiggum': 4,\n",
       " 'comic_book_guy': 5,\n",
       " 'edna_krabappel': 6,\n",
       " 'homer_simpson': 7,\n",
       " 'kent_brockman': 8,\n",
       " 'krusty_the_clown': 9,\n",
       " 'lisa_simpson': 10,\n",
       " 'marge_simpson': 11,\n",
       " 'milhouse_van_houten': 12,\n",
       " 'moe_szyslak': 13,\n",
       " 'ned_flanders': 14,\n",
       " 'nelson_muntz': 15,\n",
       " 'principal_skinner': 16,\n",
       " 'sideshow_bob': 17}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labelを離散数字にする\n",
    "import numpy as np\n",
    "class_mapping = {label:idx for idx, label in enumerate(np.unique(df['label']))}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abraham_grampa_simpson\n",
      "apu_nahasapeemapetilon\n",
      "bart_simpson\n",
      "charles_montgomery_burns\n",
      "chief_wiggum\n",
      "comic_book_guy\n",
      "edna_krabappel\n",
      "homer_simpson\n",
      "kent_brockman\n",
      "krusty_the_clown\n",
      "lisa_simpson\n",
      "marge_simpson\n",
      "milhouse_van_houten\n",
      "moe_szyslak\n",
      "ned_flanders\n",
      "nelson_muntz\n",
      "principal_skinner\n",
      "sideshow_bob\n"
     ]
    }
   ],
   "source": [
    "# labelの取得してvoc_classesを上書き\n",
    "for i in class_mapping.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strにして合体\n",
    "df['label'] = df['label'].map(class_mapping)\n",
    "df['x1'] = df['x1'].astype(str)\n",
    "df['x2'] = df['x2'].astype(str)\n",
    "df['y1'] = df['y1'].astype(str)\n",
    "df['y2'] = df['y2'].astype(str)\n",
    "df['label'] = df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df['path'] + ' ' + df['x1']+ ',' + df['y1']+ ',' + df['x2']+ ',' + df['y2'] + ',' + df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"とindexを消してtxt-fileに一度のみ実行\n",
    "#df.to_csv('train.txt',sep='\"',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create YOLOv3 model with 9 anchors and 18 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umini\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((1, 1, 1024, 69) vs (255, 1024, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\umini\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((69,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\umini\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((1, 1, 512, 69) vs (255, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\umini\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((69,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\umini\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((1, 1, 256, 69) vs (255, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\umini\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((69,) vs (255,)).\n",
      "  weight_values[i].shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights model_data/yolo.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n",
      "Train on 7101 samples, val on 788 samples, with batch size 32.\n",
      "Epoch 1/50\n",
      "221/221 [==============================] - 552s 2s/step - loss: 583.6572 - val_loss: 68.8138\n",
      "Epoch 2/50\n",
      "221/221 [==============================] - 522s 2s/step - loss: 51.4983 - val_loss: 38.9031\n",
      "Epoch 3/50\n",
      "221/221 [==============================] - 526s 2s/step - loss: 34.9591 - val_loss: 30.2472\n",
      "Epoch 4/50\n",
      "221/221 [==============================] - 529s 2s/step - loss: 28.8194 - val_loss: 26.6532\n",
      "Epoch 5/50\n",
      "221/221 [==============================] - 511s 2s/step - loss: 25.8418 - val_loss: 24.4577\n",
      "Epoch 6/50\n",
      "221/221 [==============================] - 542s 2s/step - loss: 24.0285 - val_loss: 22.9836\n",
      "Epoch 7/50\n",
      "221/221 [==============================] - 535s 2s/step - loss: 22.7987 - val_loss: 22.1876\n",
      "Epoch 8/50\n",
      "221/221 [==============================] - 539s 2s/step - loss: 22.0104 - val_loss: 21.4391\n",
      "Epoch 9/50\n",
      "221/221 [==============================] - 538s 2s/step - loss: 21.3225 - val_loss: 21.0918\n",
      "Epoch 10/50\n",
      "221/221 [==============================] - 548s 2s/step - loss: 21.0274 - val_loss: 20.7686\n",
      "Epoch 11/50\n",
      "221/221 [==============================] - 536s 2s/step - loss: 20.6625 - val_loss: 20.4777\n",
      "Epoch 12/50\n",
      "221/221 [==============================] - 524s 2s/step - loss: 20.3024 - val_loss: 20.2857\n",
      "Epoch 13/50\n",
      "221/221 [==============================] - 502s 2s/step - loss: 20.0720 - val_loss: 19.9374\n",
      "Epoch 14/50\n",
      "221/221 [==============================] - 535s 2s/step - loss: 19.8923 - val_loss: 19.8538\n",
      "Epoch 15/50\n",
      "  4/221 [..............................] - ETA: 2:57 - loss: 19.9360"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\git-test\\diveintocode-ml\\sprint18\\yolov3\\keras-yolo3\\train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[0m_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\git-test\\diveintocode-ml\\sprint18\\yolov3\\keras-yolo3\\train.py\u001b[0m in \u001b[0;36m_main\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 callbacks=[logging, checkpoint])\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'trained_weights_stage_1.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#学習\n",
    "%run train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image detection mode\n",
      " Ignoring remaining command line arguments: ./path2your_video,\n",
      "model_data/ep012-loss20.302-val_loss20.286.h5 model, anchors, and classes loaded.\n",
      "Input image filename:C:\\Users\\umini\\git-test\\diveintocode-ml\\sprint18\\yolov3\\keras-yolo3\\kaggle_simpson_testset\\bart_simpson_36.jpg\n",
      "(416, 416, 3)\n",
      "Found 0 boxes for img\n",
      "1.7085168548012426\n",
      "Input image filename:C:\\Users\\umini\\git-test\\diveintocode-ml\\sprint18\\yolov3\\keras-yolo3\\kaggle_simpson_testset\\charles_montgomery_burns_24.jpg\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "charles_montgomery_burns 0.36 (32, 0) (87, 71)\n",
      "0.08294878611443579\n",
      "Input image filename:C:\\Users\\umini\\git-test\\diveintocode-ml\\sprint18\\abraham_grampa_simpson_2.jpgyolov3\\keras-yolo3\\kaggle_simpson_testset\\\n",
      "Open Error! Try again!\n",
      "Input image filename:C:\\Users\\umini\\git-test\\diveintocode-ml\\sprint18\\abraham_grampa_simpson_2.jpgyolov3\\keras-yolo3\\kaggle_simpson_testset\\\\abraham_grampa_simpson_2.jpg\n",
      "Open Error! Try again!\n",
      "Input image filename:C:\\Users\\umini\\git-test\\diveintocode-ml\\sprint18\\yolov3\\keras-yolo3\\kaggle_simpson_testset\\abraham_grampa_simpson_2.jpg\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "abraham_grampa_simpson 0.37 (18, 22) (102, 146)\n",
      "0.07701374013288387\n"
     ]
    }
   ],
   "source": [
    "#推定\n",
    "%run yolo_video.py --image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
